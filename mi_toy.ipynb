{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/gpu/jtan/github/latent_separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateSamples(d, rho, num_sample):\n",
    "    xs = torch.randn(num_sample, d).cuda()\n",
    "    noise = torch.randn(num_sample, d).cuda()\n",
    "    ys = rho * xs + (1-rho**2)**0.5 * noise\n",
    "    xs_ys = torch.cat([xs, ys], dim=-1)\n",
    "    return xs_ys, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeTrueValues(d, axis_min, axis_max):\n",
    "    num_point = 256\n",
    "    rho = np.linspace(axis_min, axis_max, num_point)\n",
    "    mi = - np.log(1 - rho ** 2) * d / 2\n",
    "    gradient = (rho * d) / (1 - rho ** 2)\n",
    "\n",
    "    np.savez('true_{}.npz'.format(d), rho=rho, mi=mi, gradient=gradient)\n",
    "\n",
    "    print('Complete: {}-d true values.'.format(d))\n",
    "    print('  Save as: true_{}.npz'.format(d))\n",
    "    \n",
    "def _permute_dims_last_axes(joint, axes):\n",
    "    \"\"\"\n",
    "    Randomly permutes the sample along last specified axes\n",
    "    \"\"\"\n",
    "    perm = torch.clone(joint)\n",
    "    batch_size, dim_z = perm.size()\n",
    "\n",
    "    for axis in axes:\n",
    "        pi = torch.randperm(batch_size).to(perm.device)\n",
    "        perm[:, axis] = joint[pi, axis]\n",
    "\n",
    "    return perm\n",
    "\n",
    "def _shuffle_batch(z):\n",
    "    \n",
    "    batch_size, dim_z = z.size()\n",
    "    pi = torch.randperm(batch_size).to(z.device)\n",
    "    shuffled_z = z[pi]\n",
    "    return shuffled_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models import network\n",
    "from utils import distributions\n",
    "from utils.math import (log_density_gaussian, log_importance_weight_matrix, \n",
    "    matrix_log_density_gaussian, gaussian_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    MI critic\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers=4, latent_dim=10, aux_dim=1, n_units=128, activation='leaky_relu'):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        if activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        else:\n",
    "            self.activation = getattr(F, activation)\n",
    "        \n",
    "        self.dense_in = nn.Linear(latent_dim + aux_dim, n_units)\n",
    "        self.dense1 = nn.Linear(n_units, n_units)\n",
    "        self.dense2 = nn.Linear(n_units, n_units)\n",
    "        self.dense3 = nn.Linear(n_units, n_units)\n",
    "\n",
    "        # theoretically 1 with sigmoid but apparently bad results \n",
    "        # => use 2 and softmax\n",
    "        # out_units = 2\n",
    "        # self.dense_out = nn.Linear(n_units, out_units)\n",
    "\n",
    "        out_units = 1\n",
    "        self.dense_out = nn.Linear(n_units, out_units)\n",
    "        \n",
    "    \n",
    "    def forward(self, z):\n",
    "        \n",
    "        x = self.activation(self.dense_in(z))\n",
    "        x = self.activation(self.dense1(x))\n",
    "        x = self.activation(self.dense2(x))\n",
    "        x = self.activation(self.dense3(x))\n",
    "\n",
    "        out = self.dense_out(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MI_matching(nn.Module):\n",
    "    \"\"\"\n",
    "    Train critic to estimate density ratio and use density ratio estimate to construct a critic \n",
    "    for the KL lower bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, sensitive_latent_idx=[1], critic_kwargs={}, \n",
    "    optim_kwargs=dict(lr=5e-5), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.device = device\n",
    "        self.critic = Critic(**critic_kwargs).to(self.device)\n",
    "        print('Critic model for MI estimation:')\n",
    "#         print(self.critic)\n",
    "        self.CE_loss = torch.nn.BCEWithLogitsLoss(reduction='mean') # torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.opt_C = torch.optim.Adam(self.critic.parameters(), **optim_kwargs)\n",
    "\n",
    "    def call_optimize(self, latent_factors, generative_factors, sensitive_latent_idx, latent_stats, \n",
    "        storage=None, **kwargs):\n",
    "    \n",
    "        batch_size = latent_stats.size(0)\n",
    "        half_batch_size = batch_size // 2\n",
    "        generative_factors, generative_factors_critic = torch.split(generative_factors, half_batch_size, dim=0)\n",
    "        latent_factors, latent_factors_critic = torch.split(latent_factors, half_batch_size, dim=0)\n",
    "        gf_dim = generative_factors.size(1)\n",
    "\n",
    "        # generative_factors = torch.stack([generative_factors[:, sidx] for sidx in sensitive_latent_idx])\n",
    "        joint_zm = torch.cat([latent_factors, generative_factors], dim=1)\n",
    "\n",
    "        marginal_zm = torch.cat([latent_factors_critic, \n",
    "            _shuffle_batch(generative_factors_critic)], dim=1)\n",
    "\n",
    "        V_z_joint = torch.squeeze(self.critic(joint_zm))\n",
    "        V_z_marginal = torch.squeeze(self.critic(marginal_zm))\n",
    "\n",
    "        # Lower bound on MI\n",
    "        I_JS = (1. + V_z_joint.mean() - torch.exp(V_z_marginal).mean())\n",
    "        # Optimize critic\n",
    "        ones = torch.ones(half_batch_size, dtype=torch.float, device=self.device)\n",
    "        zeros = torch.zeros_like(ones)\n",
    "\n",
    "        critic_loss = 0.5 * (self.CE_loss(V_z_joint, zeros) + self.CE_loss(V_z_marginal, ones))\n",
    "        critic_loss.backward()\n",
    "        self.opt_C.step()\n",
    "        self.opt_C.zero_grad()\n",
    "\n",
    "        if storage is not None:\n",
    "            storage['I_JS'].append(supervised_term.item())\n",
    "        print('critic_loss', critic_loss.item())\n",
    "        return I_JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_MI_calc(d, range_rho, num_sample, num_epoch, n_units, GenerateData):\n",
    "    MI_approximation = []\n",
    "    gradient_approximation = []\n",
    "    device = 'cuda'\n",
    "    for rho in range_rho:\n",
    "        \n",
    "        model1 = MI_matching(device, [0], critic_kwargs={'n_units': n_units, 'latent_dim': d, 'aux_dim': d})\n",
    "        # optimizer = torch.optim.Adam(model1.parameters(), lr=1e-4)\n",
    "\n",
    "        rho = torch.FloatTensor([rho]).to(device)\n",
    "        rho.requires_grad = True\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            xs_ys, xs, ys = GenerateData(d, rho, num_sample)\n",
    "            \n",
    "            MI = -model1.call_optimize(xs, ys, [0], xs)\n",
    "            print('mi',MI.item())\n",
    "            if epoch == num_epoch - 1:\n",
    "                MI_approximation.append(MI)\n",
    "                gradient_approximation.append(-rho.grad.data)\n",
    "            else:\n",
    "                rho.grad.data.zero_()\n",
    "#                 optimizer.step()\n",
    "\n",
    "    gradient_approximation = torch.stack(gradient_approximation).view(-1).detach().cpu().numpy()\n",
    "    MI_approximation = torch.stack(MI_approximation).view(-1).detach().cpu().numpy()\n",
    "    return gradient_approximation, MI_approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMINE_F(d, axis_min, axis_max, num_lines):\n",
    "    print('Compute MINE-f(NWJ) estimation, d={}'.format(d))\n",
    "\n",
    "    num_point = 30\n",
    "    num_sample = 256\n",
    "    num_epoch = 200\n",
    "    num_units = 512\n",
    "\n",
    "    rho = np.linspace(axis_min, axis_max, num_point)\n",
    "    mi, gradient = [], []\n",
    "\n",
    "    for i in tqdm(range(num_lines)):\n",
    "        g, m = toy_MI_calc(d, rho, num_sample, num_epoch, num_units, GenerateSamples)\n",
    "        mi.append(m)\n",
    "        gradient.append(g)\n",
    "\n",
    "    mi = np.stack(mi, axis=0)\n",
    "    gradient = np.stack(gradient, axis=0)\n",
    "\n",
    "    np.savez('I_js_{}.npz'.format(d), rho=rho, mi=mi, gradient=gradient)\n",
    "    print('Complete: {} times {}-d mine-f estimation.'.format(num_lines, d))\n",
    "    print('  Save as: I_js_{}.npz'.format(d))\n",
    "    return rho, mi, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreEstimator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def rbf_kernel(self, x1, x2, kernel_width):\n",
    "        return torch.exp(\n",
    "            -torch.sum(torch.mul((x1 - x2), (x1 - x2)), dim=-1) / (2 * torch.mul(kernel_width, kernel_width))\n",
    "        )\n",
    "\n",
    "    def gram(self, x1, x2, kernel_width):\n",
    "        x_row = torch.unsqueeze(x1, -2)\n",
    "        x_col = torch.unsqueeze(x2, -3)\n",
    "        kernel_width = kernel_width[..., None, None]\n",
    "        return self.rbf_kernel(x_row, x_col, kernel_width)\n",
    "\n",
    "    def grad_gram(self, x1, x2, kernel_width):\n",
    "        x_row = torch.unsqueeze(x1, -2)\n",
    "        x_col = torch.unsqueeze(x2, -3)\n",
    "        kernel_width = kernel_width[..., None, None]\n",
    "        G = self.rbf_kernel(x_row, x_col, kernel_width)\n",
    "        diff = (x_row - x_col) / (kernel_width[..., None] ** 2)\n",
    "        G_expand = torch.unsqueeze(G, -1)\n",
    "        grad_x2 = G_expand * diff\n",
    "        grad_x1 = G_expand * (-diff)\n",
    "        return G, grad_x1, grad_x2\n",
    "\n",
    "    def heuristic_kernel_width(self, x_samples, x_basis):\n",
    "        n_samples = x_samples.size()[-2]\n",
    "        n_basis = x_basis.size()[-2]\n",
    "        x_samples_expand = torch.unsqueeze(x_samples, -2)\n",
    "        x_basis_expand = torch.unsqueeze(x_basis, -3)\n",
    "        pairwise_dist = torch.sqrt(\n",
    "            torch.sum(torch.mul(x_samples_expand - x_basis_expand, x_samples_expand - x_basis_expand), dim=-1)\n",
    "        )\n",
    "        k = n_samples * n_basis // 2\n",
    "        top_k_values = torch.topk(torch.reshape(pairwise_dist, [-1, n_samples * n_basis]), k=k)[0]\n",
    "        kernel_width = torch.reshape(top_k_values[:, -1], x_samples.size()[:-2])\n",
    "        return kernel_width.detach()\n",
    "\n",
    "    def compute_gradients(self, samples, x=None):\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralScoreEstimator(ScoreEstimator):\n",
    "    def __init__(self, n_eigen=None, eta=None, n_eigen_threshold=None):\n",
    "        self._n_eigen = n_eigen\n",
    "        self._eta = eta\n",
    "        self._n_eigen_threshold = n_eigen_threshold\n",
    "        super().__init__()\n",
    "\n",
    "    def nystrom_ext(self, samples, x, eigen_vectors, eigen_values, kernel_width):\n",
    "        M = torch.tensor(samples.size()[-2]).to(samples.device)\n",
    "        Kxq = self.gram(x, samples, kernel_width)\n",
    "        ret = torch.sqrt(M.float()) * torch.matmul(Kxq, eigen_vectors)\n",
    "        ret *= 1. / torch.unsqueeze(eigen_values, dim=-2)\n",
    "        return ret\n",
    "\n",
    "    def compute_gradients(self, samples, x=None):\n",
    "        if x is None:\n",
    "            kernel_width = self.heuristic_kernel_width(samples, samples)\n",
    "            x = samples\n",
    "        else:\n",
    "            _samples = torch.cat([samples, x], dim=-2)\n",
    "            kernel_width = self.heuristic_kernel_width(_samples, _samples)\n",
    "\n",
    "        M = samples.size()[-2]\n",
    "        Kq, grad_K1, grad_K2 = self.grad_gram(samples, samples, kernel_width)\n",
    "        if self._eta is not None:\n",
    "            Kq += self._eta * torch.eye(M)\n",
    "\n",
    "        eigen_values, eigen_vectors = torch.symeig(Kq, eigenvectors=True, upper=True)\n",
    "\n",
    "        if (self._n_eigen is None) and (self._n_eigen_threshold is not None):\n",
    "            eigen_arr = torch.mean(\n",
    "                torch.reshape(eigen_values, [-1, M]), dim=0)\n",
    "\n",
    "            eigen_arr = torch.flip(eigen_arr, [-1])\n",
    "            eigen_arr /= torch.sum(eigen_arr)\n",
    "            eigen_cum = torch.cumsum(eigen_arr, dim=-1)\n",
    "            eigen_lt = torch.lt(eigen_cum, self._n_eigen_threshold)\n",
    "            self._n_eigen = torch.sum(eigen_lt)\n",
    "        if self._n_eigen is not None:\n",
    "            eigen_values = eigen_values[..., -self._n_eigen:]\n",
    "            eigen_vectors = eigen_vectors[..., -self._n_eigen:]\n",
    "        eigen_ext = self.nystrom_ext(samples, x, eigen_vectors, eigen_values, kernel_width)\n",
    "        grad_K1_avg = torch.mean(grad_K1, dim=-3)\n",
    "        M = torch.tensor(M).to(samples.device)\n",
    "        beta = -torch.sqrt(M.float()) * torch.matmul(torch.transpose(eigen_vectors, -1, -2),\n",
    "                                                     grad_K1_avg) / torch.unsqueeze(eigen_values, -1)\n",
    "        grads = torch.matmul(eigen_ext, beta)\n",
    "        self._n_eigen = None\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_surrogate(estimator, samples):\n",
    "    dlog_q = estimator.compute_gradients(samples.detach(), None)\n",
    "    surrogate_cost = torch.mean(torch.sum(dlog_q.detach() * samples, -1))\n",
    "    return surrogate_cost\n",
    "\n",
    "\n",
    "def MIGE(d, range_rho, num_sample, GenerateData, threshold=None, n_eigen=None):\n",
    "    spectral_j = SpectralScoreEstimator(n_eigen=n_eigen, n_eigen_threshold=threshold)\n",
    "    spectral_m = SpectralScoreEstimator(n_eigen=n_eigen, n_eigen_threshold=threshold)\n",
    "    approximations = []\n",
    "    for rho in range_rho:\n",
    "        rho = torch.FloatTensor([rho]).cuda()\n",
    "        rho.requires_grad = True\n",
    "        xs_ys, xs, ys = GenerateData(d, rho, num_sample)\n",
    "\n",
    "        ans = entropy_surrogate(spectral_j, xs_ys) \\\n",
    "              - entropy_surrogate(spectral_m, ys)\n",
    "\n",
    "        ans.backward()\n",
    "        approximations.append(rho.grad.data)\n",
    "\n",
    "    approximations = torch.stack(approximations).view(-1).detach().cpu().numpy()\n",
    "    return approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMIGE(d, axis_min, axis_max, num_lines):\n",
    "    print('Compute MIGE estimation, d={}'.format(d))\n",
    "\n",
    "    num_point = 32\n",
    "    num_sample = 256\n",
    "\n",
    "    rho = np.linspace(axis_min, axis_max, num_point)\n",
    "    gradient = []\n",
    "\n",
    "    for i in tqdm(range(num_lines)):\n",
    "        g = MIGE(d, rho, num_sample, GenerateSamples, threshold=0.99)\n",
    "        gradient.append(g)\n",
    "\n",
    "    gradient = np.stack(gradient, axis=0)\n",
    "\n",
    "    np.savez('mige_{}.npz'.format(d), rho=rho, gradient=gradient)\n",
    "    print('Complete: {} times {}-d mige estimation.'.format(num_lines, d))\n",
    "    print('  Save as: mige_{}.npz'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [8, 16, 32]\n",
    "num_lines = 1\n",
    "axis_min, axis_max = -0.8, 0.8\n",
    "\n",
    "for d in ds:\n",
    "    ComputeTrueValues(d, axis_min, axis_max)\n",
    "\n",
    "for d in ds:\n",
    "    ComputeMIGE(d, axis_min, axis_max, num_lines)    \n",
    "\n",
    "for d in ds:\n",
    "    ComputeMINE_F(d, axis_min, axis_max, num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw(ds, num_lines):\n",
    "    # Plot Setting\n",
    "    plt.figure(figsize=(3.8 * len(ds), 3.6 * 2))\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "        d = ds[i]\n",
    "        true_value = np.load('true_{}.npz'.format(d))\n",
    "        true_r, true_mi, true_g = true_value['rho'], true_value['mi'], true_value['gradient']\n",
    "        mine_value = np.load('I_js_{}.npz'.format(d))\n",
    "        mine_r, mine_mi, mine_g = mine_value['rho'], mine_value['mi'], mine_value['gradient']\n",
    "        mige_value = np.load('mige_{}.npz'.format(d))\n",
    "        mige_r, mige_g = mige_value['rho'], mige_value['gradient']\n",
    "\n",
    "        plt.subplot(2, len(ds), i + 1)\n",
    "        for j in range(num_lines - 1):\n",
    "            plt.plot(mine_r, mine_mi[j], color='C1', alpha=0.1, linewidth=1.5)\n",
    "        for j in range(num_lines - 1):\n",
    "            plt.plot(mine_f_r, mine_f_mi[j], color='C2', alpha=0.1, linewidth=1.5)\n",
    "        plt.plot(true_r, true_mi, label=r'True MI', linewidth=1.5)\n",
    "        plt.plot(mine_r, mine_mi[num_lines - 1], label=r'MINE', color='C1', linewidth=1.5)\n",
    "        plt.title('d = {}'.format(d), fontsize=16)\n",
    "        plt.grid(True, linestyle='-.')\n",
    "        plt.tick_params(labelbottom=False)\n",
    "        if i == 0:\n",
    "            plt.ylabel('Mutual Information', fontsize=16)\n",
    "            plt.legend(loc='upper left', prop={'size': 12})\n",
    "\n",
    "        plt.subplot(2, len(ds), len(ds) + i + 1)\n",
    "        for j in range(num_lines - 1):\n",
    "            plt.plot(mine_r, mine_g[j], color='C1', alpha=0.1, linewidth=1.5)\n",
    "        for j in range(num_lines - 1):\n",
    "            plt.plot(mine_f_r, mine_f_g[j], color='C2', alpha=0.1, linewidth=1.5)\n",
    "        for j in range(num_lines - 1):\n",
    "            plt.plot(mige_r, mige_g[j], color='C3', alpha=0.1, linewidth=1.5)\n",
    "        plt.plot(true_r, true_g, label='True Gradient', linewidth=1.5)\n",
    "        plt.plot(mine_r, mine_g[num_lines - 1], label=r'$\\nabla_{\\rho}$ MINE', color='C1', linewidth=1.5)\n",
    "        plt.plot(mige_r, mige_g[num_lines - 1], label='MIGE (ours)', color='C3', linewidth=1.5)\n",
    "        plt.xlabel(r'$\\rho$', fontsize=16)\n",
    "        plt.grid(True, linestyle='-.')\n",
    "        if i == 0:\n",
    "            plt.ylabel('Gradient', fontsize=16)\n",
    "            plt.legend(loc='upper left', prop={'size': 12})\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./toy.pdf', format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw(ds, num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
