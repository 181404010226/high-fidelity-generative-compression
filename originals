17:22:03 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:22:07 INFO - load_model: MODEL TYPE: compression
17:22:07 INFO - load_model: MODEL MODE: evaluation
17:28:12 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:28:14 INFO - load_model: MODEL TYPE: compression
17:28:14 INFO - load_model: MODEL MODE: evaluation
17:28:44 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:28:47 INFO - load_model: MODEL TYPE: compression
17:28:47 INFO - load_model: MODEL MODE: evaluation
17:32:16 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:32:18 INFO - load_model: MODEL TYPE: compression
17:32:18 INFO - load_model: MODEL MODE: evaluation
17:33:15 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:33:17 INFO - load_model: MODEL TYPE: compression
17:33:17 INFO - load_model: MODEL MODE: evaluation
17:33:33 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:33:35 INFO - load_model: MODEL TYPE: compression
17:33:35 INFO - load_model: MODEL MODE: evaluation
17:33:36 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:33:36 INFO - load_model: Trainable parameters:
17:33:36 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:33:36 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:33:36 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:33:36 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:33:36 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:33:36 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:33:36 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:33:36 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:33:36 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:33:36 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:33:36 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:33:36 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:33:36 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:33:36 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:33:36 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:33:36 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:33:36 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:33:36 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:33:36 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:33:36 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:33:36 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:33:36 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:33:36 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:33:36 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:33:36 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:33:36 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:33:36 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:33:36 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:33:36 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:33:36 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:33:36 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:33:36 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:33:36 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:33:36 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:33:36 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:33:36 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:33:36 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:33:36 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:33:36 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:33:36 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:33:36 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:33:36 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:33:36 INFO - load_model: Number of trainable parameters: 148286543
17:33:36 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:33:36 INFO - load_model: Model init 3.697s
17:33:36 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 500000, 'name': 'low_rate_v01_openimages_compression_2020_08_18_01_57', 'noise_dim': 0, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57', 'storage_save': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints/low_rate_v01_openimages_compression_2020_08_18_01_57_epoch3_idx179906_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints/low_rate_v01_openimages_compression_2020_08_18_01_57_epoch3_idx179906_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints/low_rate_v01_openimages_compression_2020_08_18_01_57_epoch3_idx179906_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:33:37 INFO - compression_forward: Padding input image to 16
17:33:37 INFO - compression_forward: Padding latents to 4
17:35:34 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:35:37 INFO - load_model: MODEL TYPE: compression
17:35:37 INFO - load_model: MODEL MODE: evaluation
17:35:38 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:35:38 INFO - load_model: Trainable parameters:
17:35:38 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:35:38 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:35:38 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:35:38 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:35:38 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:35:38 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:35:38 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:35:38 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:35:38 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:35:38 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:35:38 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:35:38 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:35:38 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:35:38 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:35:38 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:35:38 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:35:38 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:35:38 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:35:38 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:35:38 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:35:38 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:35:38 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:35:38 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:35:38 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:35:38 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:35:38 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:35:38 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:35:38 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:35:38 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:35:38 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:35:38 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:35:38 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:35:38 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:35:38 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:35:38 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:35:38 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:35:38 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:35:38 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:35:38 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:35:38 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:35:38 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:35:38 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:35:38 INFO - load_model: Number of trainable parameters: 148286543
17:35:38 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:35:38 INFO - load_model: Model init 3.693s
17:35:38 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 500000, 'name': 'low_rate_v01_openimages_compression_2020_08_18_01_57', 'noise_dim': 0, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57', 'storage_save': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints/low_rate_v01_openimages_compression_2020_08_18_01_57_epoch3_idx179906_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints/low_rate_v01_openimages_compression_2020_08_18_01_57_epoch3_idx179906_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/low_rate_v01_openimages_compression_2020_08_18_01_57/checkpoints/low_rate_v01_openimages_compression_2020_08_18_01_57_epoch3_idx179906_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:35:39 INFO - compression_forward: Padding input image to 16
17:35:39 INFO - compression_forward: Padding latents to 4
17:35:50 INFO - compression_forward: Padding input image to 16
17:35:50 INFO - compression_forward: Padding latents to 4
17:35:55 INFO - compression_forward: Padding input image to 16
17:35:55 INFO - compression_forward: Padding latents to 4
17:35:58 INFO - compression_forward: Padding input image to 16
17:35:58 INFO - compression_forward: Padding latents to 4
17:36:02 INFO - compression_forward: Padding input image to 16
17:36:02 INFO - compression_forward: Padding latents to 4
17:36:07 INFO - compression_forward: Padding input image to 16
17:36:07 INFO - compression_forward: Padding latents to 4
17:36:12 INFO - compression_forward: Padding input image to 16
17:36:12 INFO - compression_forward: Padding latents to 4
17:36:16 INFO - compression_forward: Padding input image to 16
17:36:16 INFO - compression_forward: Padding latents to 4
17:36:21 INFO - compression_forward: Padding input image to 16
17:36:21 INFO - compression_forward: Padding latents to 4
17:36:25 INFO - compression_forward: Padding input image to 16
17:36:25 INFO - compression_forward: Padding latents to 4
17:36:30 INFO - compression_forward: Padding input image to 16
17:36:30 INFO - compression_forward: Padding latents to 4
17:36:35 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
17:36:35 INFO - compress_batch: Time elapsed: 57.025 s
17:36:35 INFO - compress_batch: Rate: 0.193 Images / s:
17:37:04 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:37:07 INFO - load_model: MODEL TYPE: compression
17:37:07 INFO - load_model: MODEL MODE: evaluation
17:37:09 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:37:09 INFO - load_model: Trainable parameters:
17:37:09 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:37:09 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:37:09 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:37:09 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:37:09 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:37:09 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:37:09 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:37:09 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:37:09 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:37:09 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:37:09 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:37:09 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:37:09 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:37:09 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:37:09 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:37:09 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:37:09 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:37:09 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:37:09 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:37:09 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:37:09 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:37:09 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:37:09 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:37:09 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:37:09 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:37:09 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:37:09 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:37:09 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:37:09 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:37:09 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:37:09 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:37:09 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:37:09 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:37:09 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:37:09 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:37:09 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:37:09 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:37:09 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:37:09 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:37:09 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:37:09 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:37:09 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:37:09 INFO - load_model: Number of trainable parameters: 148286543
17:37:09 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:37:09 INFO - load_model: Model init 5.321s
17:37:09 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_18_04_45', 'noise_dim': 0, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:37:09 INFO - compression_forward: Padding input image to 16
17:37:10 INFO - compression_forward: Padding latents to 4
17:37:21 INFO - compression_forward: Padding input image to 16
17:37:21 INFO - compression_forward: Padding latents to 4
17:37:26 INFO - compression_forward: Padding input image to 16
17:37:26 INFO - compression_forward: Padding latents to 4
17:37:28 INFO - compression_forward: Padding input image to 16
17:37:28 INFO - compression_forward: Padding latents to 4
17:37:33 INFO - compression_forward: Padding input image to 16
17:37:33 INFO - compression_forward: Padding latents to 4
17:37:37 INFO - compression_forward: Padding input image to 16
17:37:37 INFO - compression_forward: Padding latents to 4
17:37:42 INFO - compression_forward: Padding input image to 16
17:37:42 INFO - compression_forward: Padding latents to 4
17:37:47 INFO - compression_forward: Padding input image to 16
17:37:47 INFO - compression_forward: Padding latents to 4
17:37:51 INFO - compression_forward: Padding input image to 16
17:37:51 INFO - compression_forward: Padding latents to 4
17:37:56 INFO - compression_forward: Padding input image to 16
17:37:56 INFO - compression_forward: Padding latents to 4
17:38:01 INFO - compression_forward: Padding input image to 16
17:38:01 INFO - compression_forward: Padding latents to 4
17:38:06 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
17:38:06 INFO - compress_batch: Time elapsed: 56.959 s
17:38:06 INFO - compress_batch: Rate: 0.193 Images / s:
17:43:18 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:43:20 INFO - load_model: MODEL TYPE: compression
17:43:20 INFO - load_model: MODEL MODE: evaluation
17:43:22 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:43:22 INFO - load_model: Trainable parameters:
17:43:22 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:43:22 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:43:22 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:43:22 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:43:22 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:43:22 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:43:22 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:43:22 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:43:22 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:43:22 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:43:22 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:43:22 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:43:22 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:43:22 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:43:22 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:43:22 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:43:22 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:43:22 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:43:22 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:43:22 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:43:22 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:43:22 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:43:22 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:43:22 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:43:22 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:43:22 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:43:22 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:43:22 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:43:22 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:43:22 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:43:22 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:43:22 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:43:22 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:43:22 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:43:22 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:43:22 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:43:22 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:43:22 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:43:22 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:43:22 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:43:22 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:43:22 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:43:22 INFO - load_model: Number of trainable parameters: 148286543
17:43:22 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:43:22 INFO - load_model: Model init 3.711s
17:43:22 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_18_04_45', 'noise_dim': 0, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:43:22 INFO - compression_forward: Padding input image to 16
17:43:22 INFO - compression_forward: Padding latents to 4
17:43:33 INFO - compression_forward: Padding input image to 16
17:43:33 INFO - compression_forward: Padding latents to 4
17:43:38 INFO - compression_forward: Padding input image to 16
17:43:38 INFO - compression_forward: Padding latents to 4
17:43:41 INFO - compression_forward: Padding input image to 16
17:43:41 INFO - compression_forward: Padding latents to 4
17:43:45 INFO - compression_forward: Padding input image to 16
17:43:45 INFO - compression_forward: Padding latents to 4
17:43:50 INFO - compression_forward: Padding input image to 16
17:43:50 INFO - compression_forward: Padding latents to 4
17:43:55 INFO - compression_forward: Padding input image to 16
17:43:55 INFO - compression_forward: Padding latents to 4
17:43:59 INFO - compression_forward: Padding input image to 16
17:43:59 INFO - compression_forward: Padding latents to 4
17:44:04 INFO - compression_forward: Padding input image to 16
17:44:04 INFO - compression_forward: Padding latents to 4
17:44:09 INFO - compression_forward: Padding input image to 16
17:44:09 INFO - compression_forward: Padding latents to 4
17:44:14 INFO - compression_forward: Padding input image to 16
17:44:14 INFO - compression_forward: Padding latents to 4
17:44:18 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
17:44:18 INFO - compress_batch: Time elapsed: 56.847 s
17:44:18 INFO - compress_batch: Rate: 0.194 Images / s:
17:45:30 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:45:32 INFO - load_model: MODEL TYPE: compression
17:45:32 INFO - load_model: MODEL MODE: evaluation
17:45:34 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:45:34 INFO - load_model: Trainable parameters:
17:45:34 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:45:34 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:45:34 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:45:34 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:45:34 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:45:34 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:45:34 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:45:34 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:45:34 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:45:34 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:45:34 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:45:34 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:45:34 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:45:34 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:45:34 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:45:34 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:45:34 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:45:34 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:45:34 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:45:34 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:45:34 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:45:34 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:45:34 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:45:34 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:45:34 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:45:34 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:45:34 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:45:34 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:45:34 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:45:34 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:45:34 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:45:34 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:45:34 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:45:34 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:45:34 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:45:34 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:45:34 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:45:34 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:45:34 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:45:34 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:45:34 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:45:34 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:45:34 INFO - load_model: Number of trainable parameters: 148286543
17:45:34 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:45:34 INFO - load_model: Model init 3.717s
17:45:34 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_18_04_45', 'noise_dim': 0, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:45:34 INFO - compression_forward: Padding input image to 16
17:45:34 INFO - compression_forward: Padding latents to 4
17:45:46 INFO - compression_forward: Padding input image to 16
17:45:46 INFO - compression_forward: Padding latents to 4
17:45:50 INFO - compression_forward: Padding input image to 16
17:45:50 INFO - compression_forward: Padding latents to 4
17:45:53 INFO - compression_forward: Padding input image to 16
17:45:53 INFO - compression_forward: Padding latents to 4
17:45:58 INFO - compression_forward: Padding input image to 16
17:45:58 INFO - compression_forward: Padding latents to 4
17:46:02 INFO - compression_forward: Padding input image to 16
17:46:02 INFO - compression_forward: Padding latents to 4
17:46:07 INFO - compression_forward: Padding input image to 16
17:46:07 INFO - compression_forward: Padding latents to 4
17:46:12 INFO - compression_forward: Padding input image to 16
17:46:12 INFO - compression_forward: Padding latents to 4
17:46:16 INFO - compression_forward: Padding input image to 16
17:46:16 INFO - compression_forward: Padding latents to 4
17:46:21 INFO - compression_forward: Padding input image to 16
17:46:21 INFO - compression_forward: Padding latents to 4
17:46:26 INFO - compression_forward: Padding input image to 16
17:46:26 INFO - compression_forward: Padding latents to 4
17:46:31 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
17:46:31 INFO - compress_batch: Time elapsed: 56.834 s
17:46:31 INFO - compress_batch: Rate: 0.194 Images / s:
17:49:21 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:49:23 INFO - load_model: MODEL TYPE: compression
17:49:23 INFO - load_model: MODEL MODE: evaluation
17:49:25 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:49:25 INFO - load_model: Trainable parameters:
17:49:25 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:49:25 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:49:25 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:49:25 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:49:25 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:49:25 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:49:25 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:49:25 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:49:25 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:49:25 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:49:25 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:49:25 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:49:25 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:49:25 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:49:25 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:49:25 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:49:25 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:49:25 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:49:25 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:49:25 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:49:25 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:49:25 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:49:25 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:49:25 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:49:25 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:49:25 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:49:25 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:49:25 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:49:25 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:49:25 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:49:25 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:49:25 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:49:25 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:49:25 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:49:25 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:49:25 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:49:25 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:49:25 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:49:25 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:49:25 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:49:25 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:49:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:49:25 INFO - load_model: Number of trainable parameters: 148286543
17:49:25 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:49:25 INFO - load_model: Model init 3.718s
17:49:25 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_18_04_45', 'noise_dim': 0, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:49:25 INFO - compression_forward: Padding input image to 16
17:49:26 INFO - compression_forward: Padding latents to 4
17:49:37 INFO - compression_forward: Padding input image to 16
17:49:37 INFO - compression_forward: Padding latents to 4
17:49:42 INFO - compression_forward: Padding input image to 16
17:49:42 INFO - compression_forward: Padding latents to 4
17:49:45 INFO - compression_forward: Padding input image to 16
17:49:45 INFO - compression_forward: Padding latents to 4
17:49:49 INFO - compression_forward: Padding input image to 16
17:49:49 INFO - compression_forward: Padding latents to 4
17:49:54 INFO - compression_forward: Padding input image to 16
17:49:54 INFO - compression_forward: Padding latents to 4
17:49:59 INFO - compression_forward: Padding input image to 16
17:49:59 INFO - compression_forward: Padding latents to 4
17:50:03 INFO - compression_forward: Padding input image to 16
17:50:03 INFO - compression_forward: Padding latents to 4
17:50:08 INFO - compression_forward: Padding input image to 16
17:50:08 INFO - compression_forward: Padding latents to 4
17:50:12 INFO - compression_forward: Padding input image to 16
17:50:12 INFO - compression_forward: Padding latents to 4
17:50:17 INFO - compression_forward: Padding input image to 16
17:50:17 INFO - compression_forward: Padding latents to 4
17:50:22 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
17:50:22 INFO - compress_batch: Time elapsed: 57.391 s
17:50:22 INFO - compress_batch: Rate: 0.192 Images / s:
17:51:52 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:51:55 INFO - load_model: MODEL TYPE: compression
17:51:55 INFO - load_model: MODEL MODE: evaluation
17:51:56 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:51:56 INFO - load_model: Trainable parameters:
17:51:56 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:51:56 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:51:56 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:51:56 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:51:56 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:51:56 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:51:56 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:51:56 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:51:56 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:51:56 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:51:56 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:51:56 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:51:56 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:51:56 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:51:56 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:51:56 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:51:56 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:51:56 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:51:56 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:51:56 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:51:56 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:51:56 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:51:56 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:51:56 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:51:56 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:51:56 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:51:56 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:51:56 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:51:56 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:51:56 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:51:56 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:51:56 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:51:56 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:51:56 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:51:56 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:51:56 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:51:56 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:51:56 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:51:56 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:51:56 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:51:56 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:51:56 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:51:56 INFO - load_model: Number of trainable parameters: 148286543
17:51:56 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:51:56 INFO - load_model: Model init 3.714s
17:51:56 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_18_04_45', 'noise_dim': 0, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:51:57 INFO - compression_forward: Padding input image to 16
17:51:57 INFO - compression_forward: Padding latents to 4
17:52:08 INFO - compression_forward: Padding input image to 16
17:52:08 INFO - compression_forward: Padding latents to 4
17:52:13 INFO - compression_forward: Padding input image to 16
17:52:13 INFO - compression_forward: Padding latents to 4
17:52:16 INFO - compression_forward: Padding input image to 16
17:52:16 INFO - compression_forward: Padding latents to 4
17:52:20 INFO - compression_forward: Padding input image to 16
17:52:20 INFO - compression_forward: Padding latents to 4
17:52:25 INFO - compression_forward: Padding input image to 16
17:52:25 INFO - compression_forward: Padding latents to 4
17:52:56 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
17:52:58 INFO - load_model: MODEL TYPE: compression
17:52:58 INFO - load_model: MODEL MODE: evaluation
17:53:00 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:53:00 INFO - load_model: Trainable parameters:
17:53:00 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:53:00 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:53:00 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:53:00 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:53:00 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:53:00 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:53:00 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:53:00 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:53:00 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:53:00 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:53:00 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:53:00 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:53:00 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:53:00 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:53:00 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:53:00 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:53:00 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:53:00 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:53:00 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:53:00 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:53:00 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:53:00 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:53:00 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:53:00 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:53:00 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:53:00 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:53:00 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:53:00 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:53:00 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:53:00 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:53:00 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:53:00 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:53:00 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:53:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:53:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:53:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:53:00 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:53:00 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:53:00 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:53:00 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:53:00 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:53:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:53:00 INFO - load_model: Number of trainable parameters: 148286543
17:53:00 INFO - load_model: Estimated size (under fp32): 593.146 MB
17:53:00 INFO - load_model: Model init 3.716s
17:53:00 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_18_04_45', 'noise_dim': 0, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/tensorboard', 'timestamp': '2020_08_19_16:05', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'warmstart_ckpt': None, 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
17:53:00 INFO - compression_forward: Padding input image to 16
17:53:00 INFO - compression_forward: Padding latents to 4
17:53:11 INFO - compression_forward: Padding input image to 16
17:53:11 INFO - compression_forward: Padding latents to 4
17:53:16 INFO - compression_forward: Padding input image to 16
17:53:16 INFO - compression_forward: Padding latents to 4
17:53:19 INFO - compression_forward: Padding input image to 16
17:53:19 INFO - compression_forward: Padding latents to 4
17:53:23 INFO - compression_forward: Padding input image to 16
17:53:23 INFO - compression_forward: Padding latents to 4
17:53:28 INFO - compression_forward: Padding input image to 16
17:53:28 INFO - compression_forward: Padding latents to 4
17:53:33 INFO - compression_forward: Padding input image to 16
17:53:33 INFO - compression_forward: Padding latents to 4
17:53:37 INFO - compression_forward: Padding input image to 16
17:53:37 INFO - compression_forward: Padding latents to 4
17:53:42 INFO - compression_forward: Padding input image to 16
17:53:42 INFO - compression_forward: Padding latents to 4
17:53:47 INFO - compression_forward: Padding input image to 16
17:53:47 INFO - compression_forward: Padding latents to 4
17:53:52 INFO - compression_forward: Padding input image to 16
17:53:52 INFO - compression_forward: Padding latents to 4
17:53:57 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
17:53:57 INFO - compress_batch: Time elapsed: 56.966 s
17:53:57 INFO - compress_batch: Rate: 0.193 Images / s:
04:51:54 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
04:51:57 INFO - load_model: MODEL TYPE: compression
04:51:57 INFO - load_model: MODEL MODE: evaluation
04:51:59 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
04:51:59 INFO - load_model: Trainable parameters:
04:51:59 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
04:51:59 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
04:51:59 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
04:51:59 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
04:51:59 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
04:51:59 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
04:51:59 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
04:51:59 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
04:51:59 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
04:51:59 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
04:51:59 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
04:51:59 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
04:51:59 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
04:51:59 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
04:51:59 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
04:51:59 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
04:51:59 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
04:51:59 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
04:51:59 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
04:51:59 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
04:51:59 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
04:51:59 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
04:51:59 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
04:51:59 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
04:51:59 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
04:51:59 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
04:51:59 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
04:51:59 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
04:51:59 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
04:51:59 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
04:51:59 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
04:51:59 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
04:51:59 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
04:51:59 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
04:51:59 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
04:51:59 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
04:51:59 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
04:51:59 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
04:51:59 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
04:51:59 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
04:51:59 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
04:51:59 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
04:51:59 INFO - load_model: Number of trainable parameters: 148286543
04:51:59 INFO - load_model: Estimated size (under fp32): 593.146 MB
04:51:59 INFO - load_model: Model init 4.984s
04:51:59 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_19_16_13', 'noise_dim': 32, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/tensorboard', 'timestamp': '2020_08_20_04:49', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch1_idx59359_2020_08_20_04:49.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch1_idx59359_2020_08_20_04:49.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch1_idx59359_2020_08_20_04:49.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
04:52:00 INFO - compression_forward: Padding input image to 16
04:52:00 INFO - compression_forward: Padding latents to 4
04:52:05 INFO - compression_forward: Padding input image to 16
04:52:05 INFO - compression_forward: Padding latents to 4
04:52:08 INFO - compression_forward: Padding input image to 16
04:52:08 INFO - compression_forward: Padding latents to 4
04:52:09 INFO - compression_forward: Padding input image to 16
04:52:09 INFO - compression_forward: Padding latents to 4
04:52:11 INFO - compression_forward: Padding input image to 16
04:52:11 INFO - compression_forward: Padding latents to 4
04:52:14 INFO - compression_forward: Padding input image to 16
04:52:14 INFO - compression_forward: Padding latents to 4
04:52:16 INFO - compression_forward: Padding input image to 16
04:52:16 INFO - compression_forward: Padding latents to 4
04:52:18 INFO - compression_forward: Padding input image to 16
04:52:18 INFO - compression_forward: Padding latents to 4
04:52:21 INFO - compression_forward: Padding input image to 16
04:52:21 INFO - compression_forward: Padding latents to 4
04:52:23 INFO - compression_forward: Padding input image to 16
04:52:23 INFO - compression_forward: Padding latents to 4
04:52:25 INFO - compression_forward: Padding input image to 16
04:52:25 INFO - compression_forward: Padding latents to 4
04:52:35 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/out.h5
04:52:35 INFO - compress_batch: Time elapsed: 35.553 s
04:52:35 INFO - compress_batch: Rate: 0.309 Images / s:
04:55:07 INFO - logger_setup: /data/gpfs/projects/punim0011/jtan/github/high-fidelity-generative-compression/compress.py
04:55:08 INFO - load_model: MODEL TYPE: compression
04:55:08 INFO - load_model: MODEL MODE: evaluation
04:55:10 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
04:55:10 INFO - load_model: Trainable parameters:
04:55:10 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
04:55:10 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
04:55:10 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
04:55:10 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
04:55:10 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
04:55:10 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
04:55:10 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
04:55:10 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
04:55:10 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
04:55:10 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
04:55:10 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
04:55:10 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
04:55:10 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
04:55:10 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
04:55:10 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
04:55:10 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
04:55:10 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
04:55:10 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
04:55:10 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
04:55:10 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
04:55:10 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
04:55:10 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
04:55:10 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
04:55:10 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
04:55:10 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
04:55:10 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
04:55:10 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
04:55:10 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
04:55:10 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
04:55:10 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
04:55:10 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
04:55:10 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
04:55:10 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
04:55:10 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
04:55:10 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
04:55:10 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
04:55:10 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
04:55:10 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
04:55:10 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
04:55:10 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
04:55:10 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
04:55:10 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
04:55:10 INFO - load_model: Number of trainable parameters: 148286543
04:55:10 INFO - load_model: Estimated size (under fp32): 593.146 MB
04:55:10 INFO - load_model: Model init 3.579s
04:55:10 INFO - compress_batch: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 0, 'figures_save': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/figures', 'force_set_gpu': False, 'gpu': 0, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_openimages_compression_2020_08_19_16_13', 'noise_dim': 32, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': 'experiments', 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13', 'storage_save': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/tensorboard', 'timestamp': '2020_08_20_04:49', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/norm_low_rate_openimages_compression_2020_08_18_04_45/checkpoints/norm_low_rate_openimages_compression_2020_08_18_04_45_epoch2_idx166530_2020_08_19_16:05.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch1_idx59359_2020_08_20_04:49.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(batch_size=1, ckpt_path='experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch1_idx59359_2020_08_20_04:49.pt', image_dir='data/originals/CLIC/', output_dir='data/prelim/')>, 'ckpt_path': 'experiments/norm_low_rate_openimages_compression_2020_08_19_16_13/checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch1_idx59359_2020_08_20_04:49.pt', 'image_dir': 'data/originals/CLIC/', 'output_dir': 'data/prelim/'}
04:55:11 INFO - compression_forward: Padding input image to 16
04:55:11 INFO - compression_forward: Padding latents to 4
04:55:16 INFO - compression_forward: Padding input image to 16
04:55:16 INFO - compression_forward: Padding latents to 4
04:55:19 INFO - compression_forward: Padding input image to 16
04:55:19 INFO - compression_forward: Padding latents to 4
04:55:20 INFO - compression_forward: Padding input image to 16
04:55:20 INFO - compression_forward: Padding latents to 4
04:55:22 INFO - compression_forward: Padding input image to 16
04:55:22 INFO - compression_forward: Padding latents to 4
04:55:25 INFO - compression_forward: Padding input image to 16
04:55:25 INFO - compression_forward: Padding latents to 4
04:55:27 INFO - compression_forward: Padding input image to 16
04:55:27 INFO - compression_forward: Padding latents to 4
04:55:29 INFO - compression_forward: Padding input image to 16
04:55:29 INFO - compression_forward: Padding latents to 4
04:55:32 INFO - compression_forward: Padding input image to 16
04:55:32 INFO - compression_forward: Padding latents to 4
04:55:34 INFO - compression_forward: Padding input image to 16
04:55:34 INFO - compression_forward: Padding latents to 4
04:55:37 INFO - compression_forward: Padding input image to 16
04:55:37 INFO - compression_forward: Padding latents to 4
04:55:41 INFO - compress_batch: Complete. Reconstructions saved to data/prelim/. Output statistics saved to data/prelim/compression_metrics.h5
04:55:41 INFO - compress_batch: Time elapsed: 31.023 s
04:55:41 INFO - compress_batch: Rate: 0.355 Images / s:
